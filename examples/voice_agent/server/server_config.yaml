# This is an example config for setting up a NeMo Voice Agent server.
# Please refer to https://github.com/NVIDIA-NeMo/NeMo/tree/main/examples/voice_agent/README.md for more details

transport:
  audio_out_10ms_chunks: 10  # use 4 as websocket default, but increasing to a larger number might have less glitches in TTS output

vad:
  type: silero
  confidence: 0.6  # VAD threshold for detecting speech versus non-speech
  start_secs: 0.1  # min amount of speech to trigger UserStartSpeaking
  stop_secs: 0.8  # min amount of silence to trigger UserStopSpeaking
  min_volume: 0.4  # Microphone volumn threshold for VAD

stt:
  type: nemo
  model: "stt_en_fastconformer_hybrid_large_streaming_80ms"
  device: "cuda"
  att_context_size: [70, 1] # left and right attention context sizes for streaming ASR
  frame_len_in_secs: 0.08  # default for FastConformer, do not change unless using other architechtures

diar:
  type: nemo
  enabled: true # set to false to disable
  model: "nvidia/diar_streaming_sortformer_4spk-v2"
  device: "cuda"
  threshold: 0.4  # threshold value used to determine if a speaker exists or not, setting it to a lower value will increaset the sensitivity of the model
  frame_len_in_secs: 0.08  # default for Sortformer, do not change unless using other architechtures

turn_taking:
  backchannel_phrases: "./server/backchannel_phrases.yaml"  # set it to the actual path of the file, or specify a list of backchannel phrases here
  max_buffer_size: 2  # num of words more than this amount will interrupt the LLM immediately if not backchannel phrases
  bot_stop_delay: 0.5  # a delay in seconds allowed between server and client audio output, so that the BotStopSpeaking signal is handled not too far away from the actual time that the user hears all audio output

llm:
  type: hf
  dtype: bfloat16  # torch.dtype for LLM
  model: "Qwen/Qwen2.5-7B-Instruct"  # model name for HF models, will be used via `AutoModelForCausalLM.from_pretrained()`
  device: "cuda"
  system_role: "system"  # role for system prompt, set it to `user` for models that do not support system prompt
  # `system_prompt` is used as the sytem prompt to the LLM, please refer to differnt LLM webpage for spcial functions like enabling/disabling thinking
  # system_prompt: /path/to/prompt.txt  # or use path to a txt file that contains a long prompt, for example in `../example_prompts/fast_bite.txt`
  system_prompt: "You are a helpful AI agent named Lisa. Start by greeting the user warmly and introducing yourself within one sentence. Your answer should be concise and to the point. You might also see speaker tags (<speaker_0>, <speaker_1>, etc.) in the user context. You should respond to the user based on the speaker tag and the context of that speaker. Do not include the speaker tags in your response, use them only to identify the speaker. If a speaker provides their name, use their name when addressing their requests."
  # Please refer to the model page of each HF LLM model to set following params properly.
  apply_chat_template_kwargs:  # kwargs that will be passed into tokenizer.apply_chat_template() function
    add_generation_prompt: true  # This is required in most cases, do not change unless you're sure of it
    tokenize: false  # This is required, do not change
  generation_kwargs:  # kwargs that will be passed into model.generate() function of HF models
    temperature: 0.7  # LLM sampling params
    top_k: 20  # LLM sampling params
    top_p: 0.9  # LLM sampling params
    min_p: 0.0  # LLM sampling params
    max_new_tokens: 256  # max num of output tokens from LLM
    do_sample: true # enable sampling

tts:
  type: nemo
  model: fastpitch-hifigan
  fastpitch_model: "nvidia/tts_en_fastpitch"
  hifigan_model: "nvidia/tts_hifigan"
  device: "cuda"
  extra_separator:  # a list of additional punctuations to chunk LLM response into segments for faster TTS output, e.g., ",". Set to `null` to use default behavior 
    - ","
    - "?"
    - "!"
  think_tokens: ["<think>", "</think>"]  # specify them to avoid TTS for thinking process, set to `null` to allow thinking out loud
